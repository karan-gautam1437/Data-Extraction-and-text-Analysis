# -*- coding: utf-8 -*-
"""Data Extraction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13Iw12lGxBWE2aWHh8Eo5maNrxol4od_N

### Data Extractions and Text Analyis Project for Blackcoffer
"""

#importing library
import pandas as pd
import requests
from bs4 import BeautifulSoup
import numpy as np
from google.colab import files
from nltk.tokenize import word_tokenize
import nltk
import string
nltk.download('punkt')

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_excel('/content/drive/MyDrive/20211030 Test Assignment/Input.xlsx')
df

"""### Data extraction"""

from google.colab import files

# Iterate over each URL
for url_id, url in zip(df['URL_ID'], df['URL']):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'
        }
        page = requests.get(url, headers=headers)
        page.raise_for_status()  # Raise an exception for HTTP errors
        soup = BeautifulSoup(page.content, 'html.parser')
        content = soup.find(attrs={'class': 'td-post-content'}).get_text(strip=True)
        title = soup.find(attrs={'class': 'entry-title'}).get_text(strip=True)
        text = title + '.' + content

        # Save text to a file with URL_ID as filename
        file_name = f"{url_id}.txt"
        with open(file_name, 'w') as f:
            f.write(text)

        # Download the file
        files.download(file_name)

    except Exception as e:
        print(f"Error processing URL_ID {url_id}: {e}")

import pandas as pd

# List of file paths
file_paths = ["/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0001.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0002.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0003.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0004.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0005.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0006.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0007.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0008.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0009.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0010.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0011.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0012.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0013.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0014.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0015.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0016.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0017.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0018.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0019.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0020.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0021.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0022.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0023.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0024.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0025.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0026.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0027.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0028.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0029.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0030.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0031.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0032.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0033.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0034.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0035.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0037.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0038.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0039.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0040.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0041.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0042.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0043.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0044.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0045.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0046.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0047.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0048.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0050.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0051.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0052.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0053.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0054.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0055.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0056.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0057.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0058.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0059.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0060.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0061.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0062.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0063.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0064.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0065.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0066.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0067.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0068.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0069.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0070.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0071.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0072.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0073.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0074.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0075.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0076.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0077.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0078.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0079.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0080.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0081.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0082.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0083.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0084.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0085.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0086.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0087.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0088.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0089.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0090.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0091.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0092.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0093.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0094.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0095.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0096.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0097.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0098.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0099.txt",
"/content/drive/MyDrive/20211030 Test Assignment/Title Text/blackassign0100.txt",]

# Initialize an empty list to store text from each file
texts = []

# Read each text file and append its content to the 'texts' list
for file_path in file_paths:
    with open(file_path, 'r') as file:
        text = file.read()
        texts.append(text)

for text in texts:
    pass

text=str(text)

"""### Data analysis"""

#converting text to sentence
import re

sentences = re.split(r'\. ', text)

# Creating a DataFrame with the sentences
b = pd.DataFrame({'abc': sentences})

def remove_period(text):
    return text.replace('.', '')

# Apply the function to each row in the 'abc' column
b['abc'] = b['abc'].apply(remove_period)

# Replace empty strings with NaN values
c = b.replace('', np.nan)

# Remove rows containing NaN values
c.dropna(inplace=True)

# Reset the index of the DataFrame
c.reset_index(drop=True, inplace=True)

c

punc=[punc for punc in string.punctuation]

punc

StopWords_Auditor = pd.read_csv("/content/drive/MyDrive/20211030 Test Assignment/StopWords/StopWords_Auditor.txt", header=None)
StopWords_Currencies = pd.read_csv("/content/drive/MyDrive/20211030 Test Assignment/StopWords/StopWords_Currencies.txt", header=None, encoding="ISO-8859-1")
StopWords_DatesandNumbers = pd.read_csv("/content/drive/MyDrive/20211030 Test Assignment/StopWords/StopWords_DatesandNumbers.txt", header=None)
StopWords_Generic = pd.read_csv("/content/drive/MyDrive/20211030 Test Assignment/StopWords/StopWords_Generic.txt", header=None)
StopWords_GenericLong = pd.read_csv("/content/drive/MyDrive/20211030 Test Assignment/StopWords/StopWords_GenericLong.txt", header=None)
StopWords_Geographic = pd.read_csv("/content/drive/MyDrive/20211030 Test Assignment/StopWords/StopWords_Geographic.txt", header=None)
StopWords_Names = pd.read_csv("/content/drive/MyDrive/20211030 Test Assignment/StopWords/StopWords_Names.txt", header=None)

def text_process(text):
    nopunc =[char for char in text if char not in punc or char not in [':',',','(',')','’','?']]
    nopunc=''.join(nopunc)
    txt=' '.join([word for word in nopunc.split() if word.lower() not in StopWords_Auditor])
    txt1=' '.join([word for word in txt.split() if word.lower() not in StopWords_Currencies])
    txt2=' '.join([word for word in txt1.split() if word.lower() not in StopWords_DatesandNumbers])
    txt3=' '.join([word for word in txt2.split() if word.lower() not in StopWords_Generic])
    txt4=' '.join([word for word in txt3.split() if word.lower() not in StopWords_GenericLong])
    txt5=' '.join([word for word in txt4.split() if word.lower() not in StopWords_Geographic])
    return ' '.join([word for word in txt5.split() if word.lower() not in StopWords_Names])

c['abc']=c['abc'].apply(text_process)
c

#importing master Dictionary
positive=pd.read_csv("/content/drive/MyDrive/20211030 Test Assignment/MasterDictionary/positive-words.txt",header=None)
negative=pd.read_csv("/content/drive/MyDrive/20211030 Test Assignment/MasterDictionary/negative-words.txt",header=None,encoding="ISO-8859-1")

positive.columns=['abc']
negative.columns=['abc']
positive['abc']=positive['abc'].astype(str)
negative['abc']=negative['abc'].astype(str)

#positive and negative dictionary without stopwords
positive['abc']=positive['abc'].apply(text_process)
negative['abc']=negative['abc'].apply(text_process)
positive

#positive list
length=positive.shape[0]
post=[]
for i in range(0,length):
   nopunc =[char for char in positive.iloc[i] if char not in string.punctuation or char != '+']
   nopunc=''.join(nopunc)

   post.append(nopunc)

#negative list
length=negative.shape[0]
neg=[]
for i in range(0,length):
  nopunc =[char for char in negative.iloc[i] if char not in string.punctuation or char != '+']
  nopunc=''.join(nopunc)
  neg.append(nopunc)

txt_list=[]
length=c.shape[0]
for i in range(0,length):
  txt=' '.join([word for word in c.iloc[i]])
  txt_list.append(txt)

#tokenization of text
tokenize_text=[]
for i in txt_list:

  tokenize_text+=(word_tokenize(i))

print(tokenize_text)

len(tokenize_text)

"""### 1) POSITIVE SCORE"""

positive_score=0
for i in tokenize_text:
  if(i.lower() in post):
    positive_score+=1
print('postive score=', positive_score)

"""### 2) NEGATIVE SCORE"""

negative_score=0
for i in tokenize_text:
  if(i.lower() in neg):
    negative_score+=1
print('negative score=', negative_score)

"""### 3) POLARITY SCORE"""

#Polarity Score = (Positive Score – Negative Score)/ ((Positive Score + Negative Score) + 0.000001)
Polarity_Score=(positive_score-negative_score)/((positive_score+negative_score)+0.000001)
print('polarity_score=', Polarity_Score)

"""### 4) SUBJECTIVITY SCORE"""

#Subjectivity Score = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001)
subjectiivity_score=(positive_score-negative_score)/((len(tokenize_text))+ 0.000001)
print('subjectivity_score',subjectiivity_score)

"""### 5) AVG SENTENCE LENGTH"""

length=c.shape[0]
avg_length=[]
for i in range(0,length):
  avg_length.append(len(c['abc'].iloc[i]))
avg_sentence_length=sum(avg_length)/len(avg_length)
print('avg sentence length=', avg_sentence_length)

"""### 6) PERCENTAGE OF COMPLEX WORDS"""

vowels=['a','e','i','o','u']
import re
count=0
complex_Word_Count=0
for i in tokenize_text:
  x=re.compile('[es|ed]$')
  if x.match(i.lower()):
   count+=0
  else:
    for j in i:
      if(j.lower() in vowels ):
        count+=1
  if(count>2):
   complex_Word_Count+=1
  count=0

Percentage_of_Complex_words=complex_Word_Count/len(tokenize_text)
print('Percentage of complex words= ',Percentage_of_Complex_words)

"""### 7) Fog Index"""

#Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)
Fog_Index = 0.4 * (avg_sentence_length + Percentage_of_Complex_words)
print('fog index= ',Fog_Index )

"""### 8) AVERAGE NUMBER OF WORDS PER SENTENCE"""

length=c.shape[0]
avg_length=[]
for i in range(0,length):
  a=[word.split( ) for word in c.iloc[i]]
  avg_length.append(len(a[0]))
  a=0
#avg
avg_no_of_words_per_sentence=sum(avg_length)/length
print("Avg no of words per sentence= ",avg_no_of_words_per_sentence)

vowels=['a','e','i','o','u']
import re
count=0
complex_Word_Count=0
for i in tokenize_text:
  x=re.compile('[es|ed]$')
  if x.match(i.lower()):
   count+=0
  else:
    for j in i:
      if(j.lower() in vowels ):
        count+=1
  if(count>2):
   complex_Word_Count+=1
  count=0
print('Complex words count=',  complex_Word_Count)

"""### 9) COMPLEX WORD COUNT"""

word_count=len(tokenize_text)
print('Word count= ', word_count)

vowels=['a','e','i','o','u']
import re
count=0
for i in tokenize_text:
  x=re.compile('[es|ed]$')
  if x.match(i.lower()):
   count+=0
  else:
    for j in i:
      if(j.lower() in vowels ):
        count+=1
syllable_count=count
print('syllable_per_word= ',syllable_count)

"""### 10) WORD COUNT"""

pronouns=['i','we','my','ours','us' ]
import re
count=0
for i in tokenize_text:
  if i.lower() in pronouns:
   count+=1
personal_pronouns=count
print('personal pronouns= ',personal_pronouns )

"""### 11) SYLLABLE PER WORD"""

count=0
for i in tokenize_text:
  for j in i:
    count+=1
avg_word_length=count/len(tokenize_text)
print('avg word= ', avg_word_length)

data={'positive_score':positive_score,'negative_score':negative_score,'Polarity_Score':Polarity_Score,'subjectiivity_score':subjectiivity_score,'avg_sentence_length':avg_sentence_length,'Percentage_of_Complex_words':Percentage_of_Complex_words,'Fog_Index':Fog_Index,'avg_no_of_words_per_sentence':avg_no_of_words_per_sentence,'complex_Word_Count':complex_Word_Count,'word_count':word_count,'syllable_count':syllable_count,'personal_pronouns':personal_pronouns,'avg_word_length':avg_word_length}

output = pd.DataFrame(data)
output = output.append(data, ignore_index=True)
output.columns = ['positive_score', 'negative_score', 'Polarity_Score', 'subjectivity_score', 'avg_sentence_length', 'Percentage_of_Complex_words', 'Fog_Index', 'avg_no_of_words_per_sentence', 'complex_Word_Count', 'word_count', 'syllable_count', 'personal_pronouns', 'avg_word_length']

output.to_csv('output.csv', index=False)
files.download('output.csv')